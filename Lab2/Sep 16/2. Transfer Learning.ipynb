{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS470 Introduction to Artificial Intelligent\n",
    "## Deep Learning Practice \n",
    "#### Prof. Ho-Jin Choi\n",
    "#### School of Computing, KAIST\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convolutional Neural Network\n",
    "### 3-6. Feature extraction using pre-trained models\n",
    "\n",
    "The pre-trained model is a saved network that was previously trained on a large dataset, typically on a large-scale image-classificaton task. For the pre-trained model, we can either **use the pre-trained model as it is**, or **use transfer learning to customize this model** to a given task. \n",
    "\n",
    "The intuition behind transfer learning is that if a model trained on a large and general enough dataset, this model will effectively serve as a generic model of the visual world. We can then take advantage of these learned feature maps without having to start from scratch training a large model on a large dataset.\n",
    "\n",
    "In this notebook, we will try two ways to customize a pre-trained model:\n",
    "1. **Feature Extraction**: To extract meaningful features from new samples. We will simply add a new classifier, which will be trained from scratch, on top of the pretrained model so that we can repurpose the feature maps learned previously on the large dataset. Please note that we will train the final parts of model, classification part of the pre-trained model, thus, we do not have to (re)train the entire model.\n",
    "1. **Fine-tuning**: We will jointly train both the newly-added classifier layers and the last layers of the pre-trained model. This allows us to \"fine tune\" the higher-order feature representations in the pre-trained model in order to make them more relevant for the specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dataset\n",
    "\n",
    "We will first download and load the cats and dogs dataset from the [`Tensorflow Datasets`](https://www.tensorflow.org/datasets). This [`tfds`](https://www.tensorflow.org/datasets/api_docs/python/tfds) package is the easiest way to load pre-defined data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`tfds.load`](https://www.tensorflow.org/datasets/api_docs/python/tfds/load) method downloads and caches the data, and returns a [`tf.data.Dataset`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset) objects. These objects provide powerful, efficient methods for manipulating data and piping it into your model.\n",
    "\n",
    "Let's load `cats_vs_dogs` dataset and use the subsplit feature to divide it into (train, validation, test) with 80%, 10%, and 10% of the data respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load cats vs dogs image dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting [`tf.data.Dataset`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset) objects contain **(image, label) pairs**. Where the images have variable shape and 3 channels, and the label is a scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_train)\n",
    "print(raw_validation)\n",
    "print(raw_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display the first two images and labels from the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_label_name = metadata.features['label'].int2str\n",
    "\n",
    "for image, label in raw_train.take(2):\n",
    "    plt.figure()\n",
    "    plt.imshow(image)\n",
    "    plt.title(get_label_name(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the dataset\n",
    "Let's preprocess the images for the task:\n",
    "- Rescale the input channels to a range of [-1, 1]\n",
    "- Resize the images to a fixes input size\n",
    "\n",
    "First, we will define the `preprocess_example()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 160 # All images will be resized to 160x160\n",
    "\n",
    "def preprocess_example(image, label):\n",
    "    # TODO: Cast the image vector as tf.float32\n",
    "    \n",
    "    # TODO: Rescale the input channels to a range of [-1, 1]\n",
    "    \n",
    "    # TODO: Resize the images to a fixes input size\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can apply the defined function to the dataset using [`tf.data.Dataset.map()`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#map)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Apply process_example() to dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we are going to randomly shuffle the dataset using [`tf.data.Dataset.shuffle()`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#shuffle) and combine consecutive items of the dataset into batches using [`tf.data.Dataset.batch()`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#batch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "SHUFFLE_BUFFER_SIZE = 1000\n",
    "\n",
    "# TODO: Shuffle the dataset and combine them into batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect a batch of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch, label_batch = next(iter(train_batches.take(1)))\n",
    "print(image_batch.shape)\n",
    "print(label_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the model from the pre-trained convolutional neural network\n",
    "![Convolutional neural network](https://github.com/keai-kaist/CS470/blob/main/Lab2/May%206/images/cnn-architectures.png?raw=true)\n",
    "\n",
    "We are going to build the model from the MobileNet V2. First, we need to choose which layer of the model we will use for the feature extraction. Obviously, the very last classification layer is not very useful. Thus, we will use all the layers before the flatten operation. The last layer before the flatten operation is called the \"*bottleneck layer*\" and bottleneck features retain much generality as compared to the final classification layer.\n",
    "\n",
    "To do this, let's instantiate the MobileNet V2 model pre-loaded with weights trained on ImageNet. By specifying the `include_top=False` argument, we can load a network that doesn't include the classification layers at the top, which is ideal for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
    "\n",
    "# TODO: Load MobileNetV2 without the final classification layer\n",
    "base_model = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature extractor converts each 160x160x3 image to a 5x5x1280 block of features. Let' see what it does to the example batch of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_batch = base_model(image_batch)\n",
    "print(image_batch.shape,  '->', feature_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Freeze the convolutional blocks for feature extraction\n",
    "\n",
    "We will freeze the convolutional blocks of the model and use that as a feature extractor, add a classifier on top of it and train only the top-level classifier.\n",
    "\n",
    "It's important to freeze the convolutional blocks before we compile and train the model. By freezing them, we can prevent the weights in a given layer from being updated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Freeze the base model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the base model architecture\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add the classification layer\n",
    "\n",
    "To generate predictions from the block of features, average over the spatial 5x5 spatial locations, using a tf.keras.layers.GlobalAveragePooling2D layer to convert the features to a single 1280-element vector per image.\n",
    "\n",
    "To generate predictions from the block of features, let's add the following layers:\n",
    "- `tf.keras.layers.GlobalAveragePooling2D`:  Layer to convert the features to a single 1280-element vector per image\n",
    "    - This layer converts (32, 5, 5, 1280) tensor into (32, 1280) by averaging 5x5 feature maps\n",
    "- `tf.keras.layers.Dense`:  Layer to classify the input vector as cat or dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add the classification layer\n",
    "model = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile the model\n",
    "You must compile the model before training it. Since there are two classes, let's use a binary cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "\n",
    "# TODO: Compile the model with the following paramters:\n",
    "# - optimizer: RMSprop\n",
    "# - loss: binary crossentropy\n",
    "# - metrics: accuracy\n",
    "model.compile(\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2.3M parameters in MobileNet are frozen, but there are 1.3K trainable parameters in the Dense layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss0, accuracy0 = model.evaluate(validation_batches, steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"initial loss: {:.2f}\".format(loss0))\n",
    "print(\"initial accuracy: {:.2f}\".format(accuracy0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is same to randomly select the answer. So, we will check an accuracy after training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epochs = 10\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_batches,\n",
    "    epochs=initial_epochs,\n",
    "    validation_data=validation_batches,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Feature Extraction Epochs](https://github.com/keai-kaist/CS470/blob/main/Lab2/May%206/images/feature-extraction-epochs.PNG?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning curves\n",
    "\n",
    "Let's take a look at the learning curves of the training and validation accuracy/loss when using the MobileNet V2 base model as a fixed feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Feature Extraction Graph](https://github.com/keai-kaist/CS470/blob/main/Lab2/May%206/images/feature-extraction-graph.PNG?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-6. Fine tuning the model\n",
    "\n",
    "In the previous feature extraction part, we were only training a few layers on top of an MobileNet V2 base model. The weights of the pre-trained network were not updated during training.\n",
    "\n",
    "One way to increase performance even further is to train (or \"fine-tune\") the weights of the top layers of the pre-trained model alongside the training of the classifier you added. The training process will force the weights to be tuned from generic features maps to features associated specifically to our dataset.\n",
    "\n",
    "Also, we should try to fine-tune a small number of top layers rather than the whole MobileNet model. In most convolutional networks, the higher up a layer is, the more specialized it is. The first few layers learn very simple and generic features which generalize to almost all types of images. As we go higher up, the features are increasingly more specific to the dataset on which the model was trained. The goal of fine-tuning is to adapt these specialized features to work with the new dataset, rather than overwrite the generic learning.\n",
    "\n",
    "![Features by layers](images/features-by-layers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unfreeze the top layers of the model\n",
    "All we need to do is unfreeze the `base_model` and set the bottom layers be untrainable. (i.e., we will fine-tune a small number of top layers of base mode). Then, you should recompile the model (necessary for these changes to take effect), and resume training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Unfreeze the base model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look to see how many layers are in the base model\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "# Fine tune from this layer onwards\n",
    "fine_tune_at = 100\n",
    "\n",
    "# TODO: Freeze all the layers before the `fine_tune_at` layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile the model\n",
    "Let's compile the model using a much lower training rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compile the model with the following paramters:\n",
    "# - optimizer: RMSprop\n",
    "# - loss: binary crossentropy\n",
    "# - metrics: accuracy\n",
    "\n",
    "model.compile(\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_epochs = 10\n",
    "total_epochs =  initial_epochs + fine_tune_epochs\n",
    "\n",
    "history_fine_tuned = model.fit(\n",
    "    train_batches,\n",
    "     epochs=total_epochs,\n",
    "     initial_epoch=initial_epochs,\n",
    "     validation_data=validation_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fine Tuning Epochs](https://github.com/keai-kaist/CS470/blob/main/Lab2/May%206/images/fine-tuning-epochs.PNG?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning curves\n",
    "\n",
    "Let's take a look at the learning curves of the training and validation accuracy/loss, when fine tuning the last few layers of the MobileNet V2 base model and training the classifier on top of it. The validation loss is much higher than the training loss, so you may get some overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc += history_fine_tuned.history['accuracy']\n",
    "val_acc += history_fine_tuned.history['val_accuracy']\n",
    "\n",
    "loss += history_fine_tuned.history['loss']\n",
    "val_loss += history_fine_tuned.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.ylim([0.8, 1])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "          plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.ylim([0, 1.0])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "         plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fine Tuning Graph](https://github.com/keai-kaist/CS470/blob/main/Lab2/May%206/images/fine-tuning-graph.PNG?raw=true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
