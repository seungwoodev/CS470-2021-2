{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS470 Introduction to Artificial Intelligent\n",
    "## Deep Learning Practice\n",
    "#### Prof. Ho-Jin Choi\n",
    "#### School of Computing, KAIST\n",
    "\n",
    "\n",
    "## 3. Convolutional Neural Network\n",
    "#### Contents\n",
    "- Why transfer learning?\n",
    "- Case studies\n",
    "- Feature extraction using the pre-trained models\n",
    "- Fine-tuning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-4. Why transfer learning?\n",
    "\n",
    "Constructing and training your own convolutional neural network models from scratch can be hard and a long task. A common trick used in deep learning is to use a **pre-trained** model and **fine-tune** it to the specific data it will be used for.\n",
    "\n",
    "Today, we are going to discuss about **transfer learning**.  \n",
    "\n",
    "Basic idea of the **transfer learning** is that we want to transfer weights from the models trained on the other dataset.  \n",
    "As we learn at the first CNN part, CNN model is typically divided to feature extractor and classification part. Classification layer is task-specific and depends on output. So, we can't transfer weights from classification layer. However, the feature extractor trained on large scale dataset can extract general features from given input, and it can be reused to extract features from another dataset.  \n",
    "Therefore, we call feature extractor as general layers, and the weights of the feature extractor can be transferred to another model.  \n",
    "\n",
    "![FineTune](https://github.com/keai-kaist/CS470/blob/main/Lab2/May%206/images/fine-tune.png?raw=true)\n",
    "\n",
    "Also, we can divide transfer learning to **feature extraction** and **fine-tuning**.  \n",
    "\n",
    "In the case of **feature extraction**, we freeze the general layers, and only train fully-connected layers on the top of the model for our task.  \n",
    "However, **fine-tuning** unfreezes every layers and train all of them. Typically, we use very small learning rate to prevent existing parameters from overwritten. \n",
    "\n",
    "If we have small dataset, there is a high possibility of overfitting. In this case, using transfer learning can prevent overfitting because knowledge can be trasferred without the need to train it from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-5. Case studies\n",
    "\n",
    "To use the pre-trained models for our task, we will first look into several well-known CNN models. Many CNN models have been studied since the 1990s. Especially, since 2010, more advanced models have been developed  through a [ImageNet: Large scale visual recognition challenge (ILSVRC)](http://www.image-net.org/challenges/LSVRC/) in the computer vision fields such as image recognition, object detection, etc.\n",
    "\n",
    "- LeNet \n",
    "- AlexNet\n",
    "- VGG \n",
    "- MobileNet\n",
    "- Inception (GoogLeNet)\n",
    "- ResNet50 \n",
    "- Xception\n",
    "- ... more to come\n",
    "\n",
    "![ImageNetWinners](https://github.com/keai-kaist/CS470/blob/main/Lab2/May%206/images/imagenet-winners.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LeNet (Y. LeCun et al., 1989)\n",
    "\n",
    "![LeNet](https://github.com/keai-kaist/CS470/blob/main/Lab2/May%206/images/lenet.png?raw=true)\n",
    "Paper: http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\n",
    "\n",
    "- Yann LeCun et al. proposed a neural network architecture for handwritten and machine-printed character recognition in 1990s.\n",
    "- The first successful applications of CNN.\n",
    "- This model consists of 3 convolution layers, 2 pooling layers and 1 fully-connected layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AlexNet (Krizhevsky, A. et al., 2012)\n",
    "\n",
    "![AlexNet](https://github.com/keai-kaist/CS470/blob/main/Lab2/May%206/images/alexnet.png?raw=true)\n",
    "Paper: https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\n",
    "\n",
    "- The first work that popularized convolutional neural networks in computer vision.\n",
    "- This was submitted to the ImageNet ILSVRC challenge in 2012. \n",
    "- This network had a very similar architecture to LeNet, but was deeper, bigger, and featured convolutional layers stacked on top of each other.\n",
    "- This model utilizes GPU, and trained using two GPUs for a week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VGG (Simonyan, K. et al., 2014)\n",
    "##### VGG-16, VGG-19\n",
    "![VGG](https://github.com/keai-kaist/CS470/blob/main/Lab2/May%206/images/vgg.png?raw=true)\n",
    "\n",
    "Paper: https://arxiv.org/pdf/1409.1556.pdf\n",
    "\n",
    "- The runner-up in ILSVRC 2014 (VGG16)\n",
    "- Its main contribution was in showing that the depth of the network is a critical component for good performance.\n",
    "- If model has deeper layers, it is possible to have more non-linearities, which results in better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inception(v3) (GoogLeNet) (Szegedy, C. et al., 2014)\n",
    "\n",
    "![GoogLeNet](https://github.com/keai-kaist/CS470/blob/main/Lab2/May%206/images/googlenet.png?raw=true)\n",
    "\n",
    "![InceptionModule](https://github.com/keai-kaist/CS470/blob/main/Lab2/May%206/images/inception-module.png?raw=true)\n",
    "Paper: https://arxiv.org/abs/1409.4842\n",
    "\n",
    "- The winner in ILSVRC 2014\n",
    "- Its main contribution was the development of an `Inception Module` that dramatically reduced the number of parameters in the network.\n",
    "- There are also several follow-up versions to the GoogLeNet, most recently Inception-v4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNet (He, K. et al., 2015)\n",
    "\n",
    "![ResNet](https://github.com/keai-kaist/CS470/blob/main/Lab2/May%206/images/resnet.png?raw=true)\n",
    "\n",
    "![ResidualConnection](https://github.com/keai-kaist/CS470/blob/main/Lab2/May%206/images/residual-connection.png?raw=true)\n",
    "Paper: https://arxiv.org/pdf/1512.03385.pdf\n",
    "\n",
    "- The winner in ILSVRC 2015\n",
    "- It features special skip connections and a heavy use of batch normalization.\n",
    "- The skip connection enables each block to additionally learn small information, that reduce the amount of information each layer needs to learn.\n",
    "- The architecture is also missing fully connected layers at the end of the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-6. Image classification using the pre-trained models\n",
    "\n",
    "#### VGG16\n",
    "We can use the pre-trained CNN models mentioned above using the Keras API [tf.keras.applications](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/applications). (More models are available in Keras which can be found here: https://github.com/keras-team/keras-applications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-74f69a3b41c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily download and load the pre-trained VGG16 model using `tf.keras.applications.VGG16`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Download and load the pre-trained VGG16 model\n",
    "vgg16 = \n",
    "vgg16.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's download a strawberry image to classify it using the pre-trained model we just have loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --output-document=\"strawberry.jpg\" https://upload.wikimedia.org/wikipedia/commons/c/ce/Bowl_of_Strawberries.jpg\n",
    "Image.open('strawberry.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To feed an image to the pre-trained model, we first have to apply preprocesses that the model used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load and preprocess the downloaded image\n",
    "image = \n",
    "x = \n",
    "x = \n",
    "x = \n",
    "\n",
    "print('Input image shape:', x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can feed the input image to the pre-trained model and get prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Predict the image using VGG16\n",
    "predictions = \n",
    "predictions = \n",
    "\n",
    "print(f'Top-{len(predictions)} predictions:')\n",
    "for index, prediction in enumerate(predictions):\n",
    "    print(f'{index + 1}. {prediction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shonw in the prediction results, the VGG16 model predicted a class of the input as a _'strawberry'_ with highest confidence value (or probability), 0.9982."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to predict again with another image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --output-document=\"orange.jpg\" https://upload.wikimedia.org/wikipedia/commons/c/c4/Orange-Fruit-Pieces.jpg\n",
    "Image.open('orange.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load and preprocess the downloaded image\n",
    "image = \n",
    "x = \n",
    "x = \n",
    "x = \n",
    "\n",
    "print('Input image shape:', x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Predict the image using VGG16\n",
    "predictions = \n",
    "predictions = \n",
    "\n",
    "print(f'Top-{len(predictions)} predictions:')\n",
    "for index, prediction in enumerate(predictions):\n",
    "    print(f'{index + 1}. {prediction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNet50\n",
    "Similar to VGG16 model, we can use RestNet50 using the Keras API. ResNet50 is so big compared to the VGG16. Let's check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Download and load the pre-trained ResNet50 model\n",
    "resnet50 = \n",
    "resnet50.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Predict the images using ResNet50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References \n",
    "- [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556) - please cite this paper if you use the VGG models in your work.\n",
    "- [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) - please cite this paper if you use the ResNet model in your work.\n",
    "- [Rethinking the Inception Architecture for Computer Vision](http://arxiv.org/abs/1512.00567) - please cite this paper if you use the Inception v3 model in your work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
