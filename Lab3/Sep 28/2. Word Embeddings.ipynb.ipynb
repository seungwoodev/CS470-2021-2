{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS470 Introduction to Artificial Intelligence\n",
    "## Deep Learning Practice \n",
    "#### Prof. Ho-Jin Choi\n",
    "#### School of Computing, KAIST\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Recurrent Neural Networks\n",
    "### 4-2.  Word embeddings\n",
    "To handle the text data, we should convert the text into numbers first (i.e., preprocess the data). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot encodings \n",
    "First method for representing the text as numbers is an \"one-hot\" encode each word in the vocabulary. Consider the sentence \"The cat sat on the mat\". The vocabulary (or unique words) in this sentence is (cat, mat, on, sat, the). To represent each word, we will create a zero vector with length equal to the number of the vocabularies, then place a one in the index that corresponds to the word. This approach is shown in the following diagram.\n",
    "\n",
    "![One Hot Encodings](https://github.com/keai-kaist/CS470/blob/main/Lab3/May%2011/images/one-hot.png?raw=true)\n",
    "\n",
    "To create a vector that contains the encoding of the sentence, we could then concatenate the one-hot vectors for each word.\n",
    "\n",
    "However, This approach has several downsides. \n",
    "1. One-hot encoding is very inefficient. \n",
    "- A one-hot encoded vector is sparse (meaning most indicies are zero). Imagine we have 10,000 words in the vocabulary. To one-hot encode each word, we would create a vector where 99.99% of the elements are zero.\n",
    "2. One-hot encoding cannot incorporate semantics between each word.\n",
    "- Every token in one-hot encoding is equally distant away from all the others. That is,\n",
    "\n",
    "![One-hot distance](https://github.com/keai-kaist/CS470/blob/main/Lab3/May%2011/images/one-hot-distance.PNG?raw=true)\n",
    "\n",
    "\n",
    "#### Word embeddings\n",
    "Word embeddings give us a way to use an efficient, dense representation in which similar words have a similar encoding. Importantly, we do not have to specify this encoding by hand.\n",
    "\n",
    "Instead of specifying the values for the embedding manually, they are trainable parameters (weights learned by the model during training, in the same way a model learns weights for a dense layer). So, the neural network captures the token's meaning as a vector.  \n",
    "It is common to see word embeddings that are 8-dimensional (for small datasets), up to 1024-dimensions when working with large datasets. A higher dimensional embedding can capture fine-grained relationships between words, but takes more data to learn.\n",
    "\n",
    "![Word Embeddings](https://github.com/keai-kaist/CS470/blob/main/Lab3/May%2011/images/embeddings.png?raw=true)\n",
    "\n",
    "Above is a diagram for a word embedding. Each word is represented as a 4-dimensional vector of floating point values. Another way to think of an embedding is as \"lookup table\". After these weights have been learned, we can encode each word by looking up the dense vector it corresponds to in the table.\n",
    "\n",
    "- Papers related to word-embeddings\n",
    "1. CBoW, Skip-gram: https://arxiv.org/pdf/1301.3781v3.pdf\n",
    "2. GloVe: https://www.aclweb.org/anthology/D14-1162.pdf\n",
    "3. fastText: https://arxiv.org/pdf/1607.04606v2.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the Embedding Layer\n",
    "\n",
    "![Keras embedding layer](https://github.com/keai-kaist/CS470/blob/main/Lab3/May%2011/images/keras-embedding.jpg?raw=true)\n",
    "\n",
    "Keras makes it easy to use word embeddings. We will take a look at the [`tf.keras.layers.Embedding(input_dim, output_dim)`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Embedding). This layers convert positive integers (indicies) into dense vectors of fixed size.\n",
    "- `input_dim`: Size of the vocabulary. 2D tensor with shape `(batch_size, input_length)`\n",
    "- `output_dim`: Dimension of the dense embedding. 3D tensor with shape `(batch_size, input_length, output_dim)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to use Embedding() !\n",
    "\n",
    "embedding = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Embedding layer can be understood as a lookup table that maps from integer indices (which stand for specific words) to dense vectors. The dimensionality of the embedding is a parameter you can experiment with to see what works well for your problem (much in the same way you would experiment with the number of neurons in a Dense layer).\n",
    "\n",
    "When you create an Embedding layer, the weights for the embedding are randomly initialized. During training, they are gradually adjusted via backpropagation. Once trained, the learned word embeddings will roughly encode similarities between words (as they were learned for the specific problem your model is trained on)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning embeddings from scratch\n",
    "We will train a sentiment classifier on IMDB movie reviews we previously did. In the process, we will learn embeddings from scratch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = \n",
    "imdb = tf.keras.datasets.imdb\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As imported, the text of reviews is integer-encoded (each integer represents a specific word in a dictionary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Length of data:' + str(len(train_data[0])))\n",
    "print(train_data[0])\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the integers back to words\n",
    "It may be useful to know how to convert integers back to text. Here, we'll create a helper function to query a dictionary object that contains the integer to string mapping.\n",
    "\n",
    "And actually, by the default setting of [`tf.keras.datasets.imdb.load_data`](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb/load_data), integer index 1 indicates the start of a sequence and 2 indicates the out-of-vocabulary character. Then, 4 is the first index that actual words appears."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Things to do\n",
    "\n",
    "1. First, we will load imdb dictionary (word_int_dict) that maps words to an integer index\n",
    "2. We will make a dictionary that maps integer index to words (int_word_dict, which is inverse of word_int_dict)\n",
    "3. The actual words must appear from index 4 in int_word_dict\n",
    "4. We will define some indexes for special tokens (0-PAD, 1-START, 2-UNK, 3-UNUSED)\n",
    "5. Finally, we will define a function (decode_review) that can convert index array to sequence of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary mapping words to an integer index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dict class using key(words)-value(integers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Movie reviews can be different lengths. We will use the [`pad_sequences`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences) function to standardize the lengths of the reviews. Then, let's set maxlen to be close to the average of the lengths of the entire data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the average lenght of dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the lenghts of data to use it as an input of model\n",
    "maxlen = \n",
    "\n",
    "# Return the 2D Numpy array of shape (train_data, maxlen) \n",
    "# by transfoming the input data with zeros peddings \n",
    "train_data = \n",
    "\n",
    "test_data = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Length of data:' + str(len(train_data[0])))\n",
    "print(train_data[0])\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use GlobalAveragePooling1D!\n",
    "pool_func = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create simple embedding model \n",
    "\n",
    "Let's define our model using `tf.keras.Sequential`\n",
    "- The first layer is a `tf.keras.layers.Embedding` layer. This layer takes the integer-encoded vocabulary and looks up the embedding vector for each word-index (convert integers to floating values). These vectors are learned as the model trains.\n",
    "- Next, a `tf.keras.layers.GlobalAveragePooling1D` layer returns a fixed-length output vector for each example by averaging over the sequence dimension. This allows the model to handle input of variable length, in the simplest way possible.  \n",
    "![Global average pool 1D](https://github.com/keai-kaist/CS470/blob/main/Lab3/May%2011/images/global-average-pool-1d.png?raw=true)\n",
    "- This fixed-length output vector is piped through a `tf.keras.layers.Dense` with 16 hidden units.\n",
    "- The last layer is a `tf.keras.layers.Dense` with a single output node. Using the sigmoid activation function, this value is a float between 0 and 1, representing a probability (or confidence level) that the review is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = \n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "\n",
    "    \n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile and train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    \n",
    ")\n",
    "\n",
    "# Then, train the model\n",
    "history = model.fit(\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this approach our model reaches a validation accuracy of around 88%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim((0.5, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the learned embeddings\n",
    "Next, let's explore the word embeddings learned during training. To use Embedding Projector, we will dump the embedding layers as two files: \n",
    "- A file of embeddings\n",
    "- A file of meta data containing words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = model.layers[0]\n",
    "weights = embedding_layer.get_weights()[0]\n",
    "\n",
    "with open('embeddings.tsv', 'w', encoding='utf-8') as embeddings_file:\n",
    "    with open('meta.tsv', 'w', encoding='utf-8') as meta_file:\n",
    "        for word_index in range(vocab_size):\n",
    "            word = reverse_word_index[word_index]\n",
    "            embedding = weights[word_index]\n",
    "            \n",
    "            print('\\t'.join(str(x) for x in embedding), file=embeddings_file)\n",
    "            print(word, file=meta_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import files\n",
    "except ImportError:\n",
    "    pass\n",
    "else:\n",
    "    files.download('embeddings.tsv')\n",
    "    files.download('meta.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can visualize the embeddings using the [Embedding Projector](http://projector.tensorflow.org).\n",
    "\n",
    "Once you load dumped files, the embeddings you have trained will now be displayed. You can search for words to find their closest neighbors. For example, try searching for \"beautiful\". You may see neighbors like \"wonderful\". (Note: your results may be a bit different, depending on how weights were randomly initialized before training the embedding layer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
